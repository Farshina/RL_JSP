code for-
Title: A Reinforcement Learning Approach to the Dynamic Job Scheduling Problem
https://ieeexplore.ieee.org/abstract/document/9955328

This paper proposes a deep Reinforcement Learning algorithm (Actor-Critic) for solving the Job Scheduling Problem in a dynamic environment with an aim to minimize the peak instantaneous electricity consumption. The training instance is randomly reset after a certain period and the solver uses online training to adapt to the new environment.

The proposed RL-based job scheduler was tested on a modified version of benchmark industrial dataset (Industrial Demand-Side Flexibility: A Benchmark Data Set..... https://dl.acm.org/doi/10.1145/3307772.3331021). From ‘instance_0’ of the dataset, we filtered the jobs starting and ending in between time-step 800 and 899. Therefore, the modified dataset had 100 time-steps and 17 non-preemptive jobs with different arrivals, deadlines, durations, and heights. We discretized the job-heights to the next largest integers (using the ceiling operator) to make the field size finite. Thus, the field size, size ( F ) becomes {100 × 889}, 889 being the height of the tallest job. The actual height is 888.89, so there was a difference of 0.11 because of the discretization. It is considered that the heights corresponds to kilowatts (kW) and, for our application, the discretization difference is negligible. The file test_job_instance_data_selected_bin.pkl contains this dataset information.
